{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cienciasgeodesicas.ufpr.br/wp-content/uploads/2021/03/cropped-folder-site3.png\" alt=\"Drawing\"/>\n",
    "\n",
    "**Federal University of Parana, Curitiba, Brazil**\n",
    "\n",
    "Geodetic Science Graduate Programme\n",
    "\n",
    "---\n",
    "\n",
    "Authors:\n",
    " - Darlan Miranda Nunes | [ORCID- 0000-0003-3557-5341](https://orcid.org/0000-0001-5566-7919)\n",
    " - Silvana Philippi Camboim | [ORCID- 0000-0003-3557-5341](https://orcid.org/0000-0003-3557-5341)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Toponyms in OpenStreetMap: an open-source framework to investigate the relationship with intrinsic quality parameters\n",
    "\n",
    "DOI: [https://doi.org/10.1080/15230406.2025.2566807](https://doi.org/10.1080/15230406.2025.2566807)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_Hl96SfGJgH"
   },
   "source": [
    "**Aims**\n",
    "\n",
    "- To conduct a quantitative assessment of elements within OpenStreetMap (OSM) that have the 'name' attribute filled for potential categories of the Brazilian Authoritative Topographic Map; and\n",
    "\n",
    "- To investigate the most significant intrinsic quality parameters that contribute to the reliability of toponyms in OSM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeTBksgqPBen"
   },
   "source": [
    "**Brief Overview of the Proposed Methodology**\n",
    "\n",
    "- Preliminary survey of potential OpenStreetMap (OSM) tags to provide relevant toponym information to categories of interest related to Brazilian Topographic Mapping;\n",
    "\n",
    "- Execution of a quantitative analysis on collaboratively entered toponyms, utilizing homogeneous grid-based approaches; and\n",
    "\n",
    "- Assessment of intrinsic quality parameters as indicators of the reliability of toponyms in a scientific context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook 01: Retrieving data from OpenStreetMap using OHSOME API and homogeneous grid cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQ7hNL9NJvFF"
   },
   "source": [
    "### Set Output File Info ***(Manual Input Required)*** and Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output file info (AMEND MANUALLY)\n",
    "# All info is in the input file name. Check it if unsure.\n",
    "# Input files in 'data' folder, subfolder 'input_code1'.\n",
    "\n",
    "# What is the study area? (Only letters/digits/underscores, in quotes.)\n",
    "study_area = \"amsterdam\"\n",
    "\n",
    "# What is the grid size? (Only letter/digits, in quotes. No underscores!)\n",
    "grid_size = \"100m\"\n",
    "\n",
    "# Are you using the \"full\" input dataset or the \"trimmed\" dataset?\n",
    "# (Check filename if unsure. Just input \"full\" or \"trimmed\". No underscores!)\n",
    "grid_extent = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pf8XOsQb0NEm"
   },
   "outputs": [],
   "source": [
    "# Import library and some pre-installed modules\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import time\n",
    "from ipywidgets import widgets\n",
    "import folium\n",
    "from IPython.display import display\n",
    "from shapely.geometry import shape, mapping\n",
    "from pyproj import Transformer\n",
    "from shapely.ops import transform\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tDFZ8qXpB1F"
   },
   "source": [
    "### Homogeneous Grid Cells\n",
    "Statistical Grid (cell size 100 x 100m) produced by Gemeente Amsterdam:\n",
    "\n",
    "https://api.data.amsterdam.nl/v1/docs/datasets/beheerkaart/cbs_grid.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYLjW_mAgqpl"
   },
   "source": [
    "#### Import Homogeneous Grid Cells from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JGVg55nc_OGx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6182c564c244eba89e3a94897de5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('Select the GeoJSON file with grid cells:', 'Amsterdam_100_grid_trimmed.geojson', 'Amsterdam…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the statistics grid in GeoJSON format\n",
    "\n",
    "grid = None\n",
    "\n",
    "# Function for selecting and loading the GeoJSON file\n",
    "def select_file(change):\n",
    "    global grid\n",
    "    selected_file = change['new']\n",
    "    \n",
    "    if selected_file != \"Select the GeoJSON file with grid cells:\":\n",
    "        file_path = os.path.join('../data/input_code1/', selected_file)\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                grid = json.load(file)\n",
    "            print(\"File selected with success:\", selected_file)\n",
    "            print(\"File path:\", file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found:\", selected_file)\n",
    "\n",
    "# Listing available GeoJSON files\n",
    "file_list = [f for f in os.listdir('../data/input_code1/') if f.endswith('.geojson')]\n",
    "options = [\"Select the GeoJSON file with grid cells:\"] + file_list\n",
    "\n",
    "# Dropdown to select the GeoJSON file\n",
    "dropdown = widgets.Dropdown(options=options)\n",
    "dropdown.observe(select_file, names='value')\n",
    "\n",
    "# Display the dropdown\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZO44YLfRvTEa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total grid cells in GeoJSON: 1\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of grid cells in GeoJSON\n",
    "total_cells = len(grid['features'])\n",
    "print(f\"Total grid cells in GeoJSON: {total_cells}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRS transformer: RD New → WGS84\n",
    "transformer = Transformer.from_crs(\"EPSG:28992\", \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "def reproject_featurecollection_to_wgs84(fc):\n",
    "    fc = copy.deepcopy(fc)\n",
    "\n",
    "    for feature in fc[\"features\"]:\n",
    "        geom = shape(feature[\"geometry\"])\n",
    "        geom_wgs = transform(\n",
    "            lambda x, y: transformer.transform(x, y),\n",
    "            geom\n",
    "        )\n",
    "        feature[\"geometry\"] = mapping(geom_wgs)\n",
    "\n",
    "    fc[\"crs\"] = {\n",
    "        \"type\": \"name\",\n",
    "        \"properties\": {\"name\": \"EPSG:4326\"}\n",
    "    }\n",
    "\n",
    "    return fc\n",
    "\n",
    "grid = reproject_featurecollection_to_wgs84(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sKgEvhB3tSQb"
   },
   "outputs": [],
   "source": [
    "# Partition the original GeoJSON grid into subsets of up to 4 cells each\n",
    "\n",
    "# Number of cells per batch\n",
    "subset_size = 4\n",
    "\n",
    "# Split the original grid cells into subsets\n",
    "subsets = [grid['features'][i:i + subset_size] for i in range(0, len(grid['features']), subset_size)]\n",
    "\n",
    "# Create a new FeatureCollection structure for each subset and add a batch ID (\"lot_id\")\n",
    "grid_subsets = []\n",
    "for index, subset in enumerate(subsets):\n",
    "    grid_subset = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': subset,\n",
    "        'lot_id': f\"lot{index + 1}\",\n",
    "        'crs': grid['crs']\n",
    "    }\n",
    "    grid_subsets.append(grid_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VPLKJN99tkYh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subsets created: 1\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the total of subsets created\n",
    "total_subsets = len(grid_subsets)\n",
    "print(f\"Total subsets created: {total_subsets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97SI_nEJ0DhI"
   },
   "source": [
    "#### Visualize the spatial distribution of the homogeneous grid cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map center (lat, lon): [52.379265953549506, 4.888598835750396]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cc591259da48f3936d85b74a4fa1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Select a Batch:', options=(('Lot 1', 0),), value=0), Output()), _d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualises the grid cell subsets using Folium\n",
    "# The CRS used for analysis (EPSG:28992) does not work with Folium.\n",
    "# Hence, a deep copy of the grid is made and reprojected to EPSG:4326.\n",
    "\n",
    "# Compute map centroid (EPSG:4326)\n",
    "geom = shape(grid[\"features\"][0][\"geometry\"])\n",
    "centroid_coords = [geom.centroid.y, geom.centroid.x]\n",
    "\n",
    "print(\"Map center (lat, lon):\", centroid_coords)\n",
    "\n",
    "# Set name of ID field\n",
    "if grid.get(\"name\") == \"grid_AmsterdamCentrum_100m_reproj\":\n",
    "    id_field = \"bkCbsGrid100Asd\"\n",
    "elif grid.get(\"name\") == \"grid_AmsterdamCentrum_200m\":\n",
    "    id_field = \"id\"\n",
    "else:\n",
    "    id_field = \"id\"  # fallback\n",
    "\n",
    "# Plot function (called by dropdown)\n",
    "def plot_subset(subset_index):\n",
    "    subset = grid_subsets[subset_index]\n",
    "\n",
    "    # Create map\n",
    "    m = folium.Map(\n",
    "        location=centroid_coords,\n",
    "        tiles=\"OpenStreetMap\",\n",
    "        zoom_start=14\n",
    "    )\n",
    "\n",
    "    # Style\n",
    "    style = {\n",
    "        \"fillColor\": \"#8C8989\",\n",
    "        \"color\": \"#e31a1c\",\n",
    "        \"weight\": 2,\n",
    "        \"fillOpacity\": 0.6\n",
    "    }\n",
    "\n",
    "    # Add grid\n",
    "    folium.GeoJson(\n",
    "        subset,\n",
    "        name=f\"Statistical Grid - Lot {subset_index + 1}\",\n",
    "        style_function=lambda x: style,\n",
    "        tooltip=folium.GeoJsonTooltip(\n",
    "            fields=[id_field],\n",
    "            aliases=[\"Grid Cell ID\"]\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "    display(m)\n",
    "\n",
    "# Dropdown widget \n",
    "dropdown = widgets.Dropdown(\n",
    "    options=[(f\"Lot {i + 1}\", i) for i in range(len(grid_subsets))],\n",
    "    description=\"Select a Batch:\",\n",
    ")\n",
    "\n",
    "widgets.interactive(plot_subset, subset_index=dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LloQGliN0l_q"
   },
   "source": [
    "### **OHSOME API**\n",
    "\n",
    " - Access to features, attributes and OSM history edits using the OHSOME API (*OpenStreetMap History Data Analytics Platform*)\n",
    "\n",
    "> - https://docs.ohsome.org/ohsome-api/v1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r9WR94F9HEMA"
   },
   "outputs": [],
   "source": [
    "# URL of OHSOME API Metadata endpoint\n",
    "URL = 'https://api.ohsome.org/v1/metadata'\n",
    "\n",
    "# Request to the OHSOME API\n",
    "response = requests.get(URL)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Request failed\")\n",
    "    print(\"Status:\", response.status_code)\n",
    "    print(\"Content-Type:\", response.headers.get(\"Content-Type\"))\n",
    "    print(response.text[:800])\n",
    "else:\n",
    "    response_json = response.json()\n",
    "    response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os16ACqcnEDi"
   },
   "source": [
    "### Retrieving data from OpenStreetMap using OHSOME API and homogeneous grid cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0nZYB2FXFZY"
   },
   "source": [
    "#### Step 1 (*API Endpoint: Elements Aggregation*): count the number of OSM features (elements) and calculate the proportion of features with the attribute \"name\" fill in by contributors, for each grid cells:\n",
    "\n",
    "\n",
    " - Determine the total number of OSM features for interest tags, grouped by grid cell;\n",
    "\n",
    " - Quantify the total number of features with attribute \"name\" filled in; and\n",
    "\n",
    " - Calculate the proportion of features with attribute \"name\" filled in for each grid cell.\n",
    "\n",
    " - Period of data retrieved: 2007-10-08 to 2024-03-10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OJ6q3oda9UEV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lot1 successfully processed!\n",
      "lot2 successfully processed!\n",
      "lot3 successfully processed!\n",
      "lot4 successfully processed!\n",
      "lot5 successfully processed!\n",
      "lot6 successfully processed!\n",
      "lot7 successfully processed!\n",
      "lot8 successfully processed!\n",
      "lot9 successfully processed!\n",
      "lot10 successfully processed!\n",
      "lot11 successfully processed!\n",
      "lot12 successfully processed!\n",
      "lot13 successfully processed!\n",
      "lot14 successfully processed!\n",
      "lot15 successfully processed!\n",
      "lot16 successfully processed!\n",
      "lot17 successfully processed!\n",
      "lot18 successfully processed!\n",
      "lot19 successfully processed!\n",
      "lot20 successfully processed!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and name=*\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     response \u001b[38;5;241m=\u001b[39m post_with_retry(url_tag, data\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m     78\u001b[0m     data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     79\u001b[0m     name_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(res\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroupByResult\u001b[39m\u001b[38;5;124m'\u001b[39m, [])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m, []))\n",
      "Cell \u001b[0;32mIn [11], line 16\u001b[0m, in \u001b[0;36mpost_with_retry\u001b[0;34m(url, data, max_retries)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# no timeout\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# raises HTTPError for 4xx/5xx\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:1348\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Approach for processing batches of 04 cells from the original grid\n",
    "\n",
    "# Step 1 (API Endpoint: Elements Aggregation): count the number of OSM features\n",
    "# (elements) and calculate the proportion of features with \"name\" attribute filled by contributors, for each grid cells:\n",
    "# Aggregation method: count\n",
    "# POST /elements/(aggregation)/groupBy/boundary/groupBy/tag\n",
    "\n",
    "def post_with_retry(url, data, max_retries=5):\n",
    "    \"\"\"\n",
    "    POST to the given URL with retries for network errors or timeouts.\n",
    "    Exponential backoff: waits 2,4,8,... seconds between attempts.\n",
    "    Waits indefinitely for the response (no timeout).\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.post(url, data=data)  # no timeout\n",
    "            response.raise_for_status()  # raises HTTPError for 4xx/5xx\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"Attempt {attempt} failed: {e}. Retrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Failed to connect after {max_retries} attempts\")\n",
    "\n",
    "# Start the time counter\n",
    "start_time = time.time()\n",
    "\n",
    "# Load a copy of previously created grid_subsets\n",
    "grid_subset2 = grid_subsets.copy()\n",
    "\n",
    "# OHSOME API endpoint url\n",
    "url_tag = \"https://api.ohsome.org/v1/elements/count/groupBy/boundary/groupBy/tag\"\n",
    "\n",
    "# OSM tags of interest\n",
    "tags_de_interesse = {\n",
    "    'leisure': '*',\n",
    "    'building': '*',\n",
    "    'amenity': '*'\n",
    "}\n",
    "\n",
    "# Configuring basic parameters\n",
    "params_base = {\n",
    "    'time': '2007-10-08/2024-03-10'\n",
    "}\n",
    "\n",
    "# List to store the final results\n",
    "final_results = {}\n",
    "\n",
    "# Process each batch of grid_subsets\n",
    "for lot_id, subset in enumerate(grid_subset2, start=1):\n",
    "    for feature in subset['features']:\n",
    "        cell_geojson = json.dumps({\"type\": \"FeatureCollection\", \"features\": [feature]})\n",
    "        cell_id = feature['properties']['id']\n",
    "\n",
    "        for tag, value in tags_de_interesse.items():\n",
    "            # 1st: Aggregate the object of each interest tag by grid cell\n",
    "            params = params_base.copy()\n",
    "            params.update({\n",
    "                'bpolys': cell_geojson,\n",
    "                'filter': f'{tag}={value}',\n",
    "                'groupByKey': tag,\n",
    "                'groupByValues': value\n",
    "            })\n",
    "\n",
    "            try:\n",
    "                response = post_with_retry(url_tag, data=params)\n",
    "                data = response.json()\n",
    "                total_count = sum(res.get('value', 0) for res in data.get('groupByResult', [])[0].get('result', []))\n",
    "                feature['properties'][f'{tag}_total_count'] = total_count\n",
    "            except RuntimeError:\n",
    "                print(f\"Warning: request failed for cell {cell_id}, tag {tag} after multiple attempts\")\n",
    "                total_count = 0\n",
    "\n",
    "            # 2nd: Count the features with the attribute 'name' filled in\n",
    "            params['filter'] = f'{tag}={value} and name=*'\n",
    "            try:\n",
    "                response = post_with_retry(url_tag, data=params)\n",
    "                data = response.json()\n",
    "                name_count = sum(res.get('value', 0) for res in data.get('groupByResult', [])[0].get('result', []))\n",
    "                feature['properties'][f'{tag}_name_count'] = name_count\n",
    "                name_ratio_perc = (name_count / total_count) * 100 if total_count > 0 else 0\n",
    "                feature['properties'][f'{tag}_name_ratio'] = name_ratio_perc\n",
    "            except RuntimeError:\n",
    "                print(f\"Warning: request failed for cell {cell_id}, tag {tag} after multiple attempts\")\n",
    "                name_count = 0\n",
    "                \n",
    "        # Add cell results to final_results\n",
    "        final_results[cell_id] = feature['properties']\n",
    "\n",
    "    print(f\"{subset['lot_id']} successfully processed!\")\n",
    "\n",
    "# Stop the time counter\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and display the total execution time\n",
    "total_time_seconds = end_time - start_time\n",
    "print(f\"Total execution time: {total_time_seconds // 60} minutes and {total_time_seconds % 60} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or append step results to a GeoJSON file\n",
    "\n",
    "# Output file using your naming convention\n",
    "output_filename = f\"../data/output_code1/{study_area}_{grid_size}_{grid_extent}_results.geojson\"\n",
    "\n",
    "# Collect current step features into a dict keyed by cell ID\n",
    "current_features = {}\n",
    "\n",
    "for subset in grid_subset2:\n",
    "    for feature in subset['features']:\n",
    "        cell_id = feature['properties']['id']\n",
    "        current_features[cell_id] = feature\n",
    "\n",
    "\n",
    "if os.path.exists(output_filename):\n",
    "    # Load existing data\n",
    "    with open(output_filename, 'r', encoding='utf-8') as f:\n",
    "        existing_fc = json.load(f)\n",
    "\n",
    "    # Index existing features by cell ID\n",
    "    existing_features = {\n",
    "        feat['properties']['id']: feat\n",
    "        for feat in existing_fc['features']\n",
    "    }\n",
    "\n",
    "    # Merge: update properties of existing features\n",
    "    for cell_id, new_feature in current_features.items():\n",
    "        if cell_id in existing_features:\n",
    "            existing_features[cell_id]['properties'].update(\n",
    "                new_feature['properties']\n",
    "            )\n",
    "        else:\n",
    "            # New cell (should not usually happen, but safe)\n",
    "            existing_features[cell_id] = new_feature\n",
    "\n",
    "    # Rebuild FeatureCollection\n",
    "    merged_fc = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'crs': existing_fc.get('crs'),\n",
    "        'features': list(existing_features.values())\n",
    "    }\n",
    "\n",
    "else:\n",
    "    # First step: just save everything\n",
    "    merged_fc = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'crs': grid_subset2[0]['crs'],\n",
    "        'features': list(current_features.values())\n",
    "    }\n",
    "\n",
    "# Write to disk\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_fc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Results merged and saved to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYN2uef0KcMb"
   },
   "source": [
    "#### Step 2 (*API Endpoint: Contributions Aggregation*): count the total number of contributions for features with and without the attribute \"name\" filled in:\n",
    "\n",
    "- Count the **total number of contributions** to the *interest tags* for the total features in the grid cells, with and without the attribute \"name\" filled in.\n",
    "\n",
    "- Period of data retrieved: 2007-10-08 to 2024-03-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lot1 successfully processed!\n",
      "Total execution time: 0.0 minutes and 53.59541726112366 seconds\n"
     ]
    }
   ],
   "source": [
    "# Step 2 (API Endpoint: Endpoint Contributions Aggregation): count the total number of\n",
    "# contributions for features with and without a name attribute filled in.\n",
    "# Aggregation method: count \n",
    "# POST /contributions/count/groupBy/boundary\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "url_contributions = \"https://api.ohsome.org/v1/contributions/count/groupBy/boundary\"\n",
    "\n",
    "tags_de_interesse = {\n",
    "    'leisure': '*',\n",
    "    'building': '*',\n",
    "    'amenity': '*'\n",
    "}\n",
    "\n",
    "params_contributions_base = {\n",
    "    'time': '2021-03-11/2024-03-10'\n",
    "}\n",
    "\n",
    "def process_response(response, cell_id):\n",
    "    data = response.json()\n",
    "    return sum(result.get('value', 0)\n",
    "               for result in data.get('groupByResult', [])[0].get('result', []))\n",
    "\n",
    "\n",
    "for lot_id, subset in enumerate(grid_subset2, start=1):\n",
    "    for feature in subset['features']:\n",
    "        cell_geojson = json.dumps({\"type\": \"FeatureCollection\", \"features\": [feature]})\n",
    "        cell_id = feature['properties']['id']\n",
    "\n",
    "        for tag in tags_de_interesse:\n",
    "\n",
    "            # --- All contributions ---\n",
    "            params_contributions_all = params_contributions_base.copy()\n",
    "            params_contributions_all.update({\n",
    "                'bpolys': cell_geojson,\n",
    "                'filter': f'{tag}=*'\n",
    "            })\n",
    "\n",
    "            try:\n",
    "                response_all = post_with_retry(url_contributions, params_contributions_all)\n",
    "                contributions_all = process_response(response_all, cell_id)\n",
    "            except RuntimeError:\n",
    "                print(f\"Skipping cell {cell_id} for tag {tag} (connection failed)\")\n",
    "                contributions_all = 0\n",
    "\n",
    "            # --- Named contributions ---\n",
    "            params_contributions_name = params_contributions_base.copy()\n",
    "            params_contributions_name.update({\n",
    "                'bpolys': cell_geojson,\n",
    "                'filter': f'{tag}=* and name=*'\n",
    "            })\n",
    "\n",
    "            try:\n",
    "                response_name = post_with_retry(url_contributions, params_contributions_name)\n",
    "                contributions_name = process_response(response_name, cell_id)\n",
    "            except RuntimeError:\n",
    "                print(f\"Skipping name contributions for cell {cell_id}, tag {tag}\")\n",
    "                contributions_name = 0\n",
    "\n",
    "            feature['properties'][f'{tag}_total_contributions'] = contributions_all\n",
    "            feature['properties'][f'{tag}_name_contributions'] = contributions_name\n",
    "\n",
    "    print(f\"{subset['lot_id']} successfully processed!\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time_seconds = end_time - start_time\n",
    "print(f\"Total execution time: {total_time_seconds // 60} minutes and {total_time_seconds % 60} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results merged and saved to: ../data/output_code1/Amsterdam_100m_full_results.geojson\n"
     ]
    }
   ],
   "source": [
    "# Save or append step results to a GeoJSON file\n",
    "\n",
    "# Collect current step features into a dict keyed by cell ID\n",
    "current_features = {}\n",
    "\n",
    "for subset in grid_subset2:\n",
    "    for feature in subset['features']:\n",
    "        cell_id = feature['properties']['id']\n",
    "        current_features[cell_id] = feature\n",
    "\n",
    "\n",
    "if os.path.exists(output_filename):\n",
    "    # Load existing data\n",
    "    with open(output_filename, 'r', encoding='utf-8') as f:\n",
    "        existing_fc = json.load(f)\n",
    "\n",
    "    # Index existing features by cell ID\n",
    "    existing_features = {\n",
    "        feat['properties']['id']: feat\n",
    "        for feat in existing_fc['features']\n",
    "    }\n",
    "\n",
    "    # Merge: update properties of existing features\n",
    "    for cell_id, new_feature in current_features.items():\n",
    "        if cell_id in existing_features:\n",
    "            existing_features[cell_id]['properties'].update(\n",
    "                new_feature['properties']\n",
    "            )\n",
    "        else:\n",
    "            # New cell (should not usually happen, but safe)\n",
    "            existing_features[cell_id] = new_feature\n",
    "\n",
    "    # Rebuild FeatureCollection\n",
    "    merged_fc = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'crs': existing_fc.get('crs'),\n",
    "        'features': list(existing_features.values())\n",
    "    }\n",
    "\n",
    "else:\n",
    "    # First step: just save everything\n",
    "    merged_fc = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'crs': grid_subset2[0]['crs'],\n",
    "        'features': list(current_features.values())\n",
    "    }\n",
    "\n",
    "# Write to disk\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_fc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Results merged and saved to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GYWrij3Jgcl"
   },
   "source": [
    "#### Step 3 (*API Endpoint: Contributions Aggregation*): Count the number of contributions in the past five years for features with the attribute \"name\" filled in:\n",
    "\n",
    " - Count the number of contributions in the past five years for tags of interest, aggregated by grid cells, with the attribute \"name\" filled in;\n",
    "\n",
    " - Period of data retrieved: 2019-03-09 to 2024-03-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HfeW9E19UXvz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lot1 successfully processed!\n",
      "Total execution time: 0.0 minutes and 26.292473793029785 seconds\n"
     ]
    }
   ],
   "source": [
    "# Step 3 (API Endpoint: Contributions Aggregation)\n",
    "# Count the number of contributions in the past five years\n",
    "# for features with a filled-in name\n",
    "# POST /contributions/latest/count\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "url_latest_contributions = \"https://api.ohsome.org/v1/contributions/latest/count\"\n",
    "\n",
    "tags_de_interesse = {\n",
    "    'leisure': '*',\n",
    "    'building': '*',\n",
    "    'amenity': '*'\n",
    "}\n",
    "\n",
    "params_contributions_base = {\n",
    "    'time': '2019-03-09/2024-03-10'\n",
    "}\n",
    "\n",
    "def process_response(response, cell_id):\n",
    "    data = response.json()\n",
    "    latest_result = data.get('result', [])\n",
    "    return latest_result[-1].get('value', 0) if latest_result else 0\n",
    "\n",
    "\n",
    "for lot_id, subset in enumerate(grid_subset2, start=1):\n",
    "    for feature in subset['features']:\n",
    "        cell_geojson = json.dumps({\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": [feature]\n",
    "        })\n",
    "        cell_id = feature['properties']['id']\n",
    "\n",
    "        for tag in tags_de_interesse:\n",
    "            params_latest_contributions = params_contributions_base.copy()\n",
    "            params_latest_contributions.update({\n",
    "                'bpolys': cell_geojson,\n",
    "                'filter': f'{tag}=* and name=*'\n",
    "            })\n",
    "\n",
    "            try:\n",
    "                response = post_with_retry(\n",
    "                    url_latest_contributions,\n",
    "                    params_latest_contributions\n",
    "                )\n",
    "                latest_contributions_count = process_response(response, cell_id)\n",
    "            except RuntimeError:\n",
    "                print(f\"Skipping cell {cell_id} for tag {tag} (connection failed)\")\n",
    "                latest_contributions_count = 0\n",
    "\n",
    "            feature['properties'][f'{tag}_latest5_name_contributions'] = (\n",
    "                latest_contributions_count\n",
    "            )\n",
    "\n",
    "    print(f\"{subset['lot_id']} successfully processed!\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time_seconds = end_time - start_time\n",
    "print(\n",
    "    f\"Total execution time: {total_time_seconds // 60} minutes \"\n",
    "    f\"and {total_time_seconds % 60} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or append step results to a GeoJSON file\n",
    "\n",
    "# Create a FeatureCollection from the current step\n",
    "current_step_fc = {\n",
    "    'type': 'FeatureCollection',\n",
    "    'crs': grid_subset2[0]['crs'],\n",
    "    'features': []\n",
    "}\n",
    "\n",
    "for subset in grid_subset2:\n",
    "    current_step_fc['features'].extend(subset['features'])\n",
    "\n",
    "# Check if file already exists\n",
    "if os.path.exists(output_filename):\n",
    "    # Load existing features\n",
    "    with open(output_filename, 'r', encoding='utf-8') as f:\n",
    "        existing_data = json.load(f)\n",
    "    # Append current step features\n",
    "    existing_data['features'].extend(current_step_fc['features'])\n",
    "    combined_fc = existing_data\n",
    "else:\n",
    "    combined_fc = current_step_fc\n",
    "\n",
    "# Save combined results back to the file\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_fc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Step results appended and saved to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCi1CXgvXbRD"
   },
   "source": [
    "#### Step 4 (*API Endpoint: Contributions Aggregation*): Count the total number of contributions to features with a filled-in name where a tagChange occurred:\n",
    "\n",
    "- Count the total number of contributions to the tags of interest, aggregated by grid cell, with the attribute name filled in, considering the type of contribution (contributionType) tag change ('tagChange').\n",
    "\n",
    "  - *contributionType available: ‘creation’, ‘deletion’, ‘tagChange’, ‘geometryChange’ ou uma combinação destes*\n",
    "\n",
    "- Period of data retrieved: 2007-10-08 to 2024-03-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6sn1XbIjVRVr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lot1 successfully processed!\n"
     ]
    }
   ],
   "source": [
    "# Step 4 (API Endpoint: Contributions Aggregation)\n",
    "# Count total contributions with tagChange\n",
    "# for features with a filled-in name\n",
    "# POST /contributions/count/groupBy/boundary\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "url_contributions = \"https://api.ohsome.org/v1/contributions/count/groupBy/boundary\"\n",
    "\n",
    "tags_de_interesse = {\n",
    "    'leisure': '*',\n",
    "    'building': '*',\n",
    "    'amenity': '*'\n",
    "}\n",
    "\n",
    "params_contributions_base = {\n",
    "    'time': '2007-10-08/2024-03-10',\n",
    "    'contributionType': 'tagChange'\n",
    "}\n",
    "\n",
    "def process_response(response, cell_id):\n",
    "    data = response.json()\n",
    "    return sum(\n",
    "        result.get('value', 0)\n",
    "        for result in data.get('groupByResult', [])[0].get('result', [])\n",
    "    )\n",
    "\n",
    "\n",
    "for lot_id, subset in enumerate(grid_subset2, start=1):\n",
    "    for feature in subset['features']:\n",
    "        cell_geojson = json.dumps({\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": [feature]\n",
    "        })\n",
    "        cell_id = feature['properties']['id']\n",
    "\n",
    "        for tag in tags_de_interesse:\n",
    "            params_contributions = params_contributions_base.copy()\n",
    "            params_contributions.update({\n",
    "                'bpolys': cell_geojson,\n",
    "                'filter': f'{tag}=* and name=*'\n",
    "            })\n",
    "\n",
    "            try:\n",
    "                response = post_with_retry(\n",
    "                    url_contributions,\n",
    "                    params_contributions\n",
    "                )\n",
    "                contributions_count = process_response(response, cell_id)\n",
    "            except RuntimeError:\n",
    "                print(\n",
    "                    f\"Skipping cell {cell_id} for tag {tag} \"\n",
    "                    f\"(connection failed)\"\n",
    "                )\n",
    "                contributions_count = 0\n",
    "\n",
    "            feature['properties'][f'{tag}_name_tagChange_contributions'] = contributions_count\n",
    "\n",
    "    print(f\"{subset['lot_id']} successfully processed!\")\n",
    "\n",
    "# Stop the time counter\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or append step results to a GeoJSON file\n",
    "\n",
    "# Create a FeatureCollection from the current step\n",
    "current_step_fc = {\n",
    "    'type': 'FeatureCollection',\n",
    "    'crs': grid_subset2[0]['crs'],\n",
    "    'features': []\n",
    "}\n",
    "\n",
    "for subset in grid_subset2:\n",
    "    current_step_fc['features'].extend(subset['features'])\n",
    "\n",
    "# Check if file already exists\n",
    "if os.path.exists(output_filename):\n",
    "    # Load existing features\n",
    "    with open(output_filename, 'r', encoding='utf-8') as f:\n",
    "        existing_data = json.load(f)\n",
    "    # Append current step features\n",
    "    existing_data['features'].extend(current_step_fc['features'])\n",
    "    combined_fc = existing_data\n",
    "else:\n",
    "    combined_fc = current_step_fc\n",
    "\n",
    "# Save combined results back to the file\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_fc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Step results appended and saved to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJFXBRuTZHZ2"
   },
   "source": [
    "#### Step 5 (API Endpoint: Users Aggregation): Count the number of users (contributors) who edited features with attribute name filled in:\n",
    "\n",
    "- Count the number of users who edited features of the OSM tags of Interest with attribute \"name\" attribute filled in, aggregated by grid cells.\n",
    "\n",
    "- Period of data retrieved: 2007-10-08 to 2024-03-10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzijjl_tVvjG"
   },
   "outputs": [],
   "source": [
    "# Step 5 (API Endpoint: Users Aggregation)\n",
    "# Count the number of users (contributors)\n",
    "# who edited features with attribute \"name\" filled in\n",
    "# POST /users/count/groupBy/boundary\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "url_users_count = \"https://api.ohsome.org/v1/users/count/groupBy/boundary\"\n",
    "\n",
    "tags_de_interesse = {\n",
    "    'leisure': '*',\n",
    "    'building': '*',\n",
    "    'amenity': '*'\n",
    "}\n",
    "\n",
    "params_users_count_base = {\n",
    "    'time': '2007-10-08/2024-03-10'\n",
    "}\n",
    "\n",
    "def process_user_response(response, cell_id):\n",
    "    data = response.json()\n",
    "    for result in data.get('groupByResult', []):\n",
    "        if result.get('groupByObject') == cell_id:\n",
    "            return result.get('result', [{}])[0].get('value', 0)\n",
    "    return 0\n",
    "\n",
    "\n",
    "for lot_id, subset in enumerate(grid_subset2, start=1):\n",
    "    for feature in subset['features']:\n",
    "        cell_geojson = json.dumps({\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": [feature]\n",
    "        })\n",
    "        cell_id = feature['properties']['id']\n",
    "\n",
    "        for tag in tags_de_interesse:\n",
    "            params_users_count = params_users_count_base.copy()\n",
    "            params_users_count.update({\n",
    "                'bpolys': cell_geojson,\n",
    "                'filter': f'{tag}=* and name=*'\n",
    "            })\n",
    "\n",
    "            try:\n",
    "                response = post_with_retry(\n",
    "                    url_users_count,\n",
    "                    params_users_count\n",
    "                )\n",
    "                users_name_count = process_user_response(response, cell_id)\n",
    "            except RuntimeError:\n",
    "                print(\n",
    "                    f\"Skipping cell {cell_id} for tag {tag} \"\n",
    "                    f\"(connection failed)\"\n",
    "                )\n",
    "                users_name_count = 0\n",
    "\n",
    "            feature['properties'][\n",
    "                f'{tag}_users_count_name'\n",
    "            ] = users_name_count\n",
    "\n",
    "    print(f\"{subset['lot_id']} successfully processed!\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time_seconds = end_time - start_time\n",
    "print(\n",
    "    f\"Total execution time: {total_time_seconds // 60} minutes \"\n",
    "    f\"and {total_time_seconds % 60} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xi8jUHHrnJoT"
   },
   "source": [
    "#### Save the updated grid cells with the information Extracted using the OHSOME API endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5c8kBm3PZ3S"
   },
   "outputs": [],
   "source": [
    "# Save or append step results to a GeoJSON file\n",
    "\n",
    "# Create a FeatureCollection from the current step\n",
    "current_step_fc = {\n",
    "    'type': 'FeatureCollection',\n",
    "    'crs': grid_subset2[0]['crs'],\n",
    "    'features': []\n",
    "}\n",
    "\n",
    "for subset in grid_subset2:\n",
    "    current_step_fc['features'].extend(subset['features'])\n",
    "\n",
    "# Check if file already exists\n",
    "if os.path.exists(output_filename):\n",
    "    # Load existing features\n",
    "    with open(output_filename, 'r', encoding='utf-8') as f:\n",
    "        existing_data = json.load(f)\n",
    "    # Append current step features\n",
    "    existing_data['features'].extend(current_step_fc['features'])\n",
    "    combined_fc = existing_data\n",
    "else:\n",
    "    combined_fc = current_step_fc\n",
    "\n",
    "# Save combined results back to the file\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_fc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Step results appended and saved to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44z-9VjZoMHO"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1DXNxbRx7WfU5LmjbnyLFPp3WsdF0TXzQ",
     "timestamp": 1702061456474
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
